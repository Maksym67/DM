{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Лабораторна робота 6: Пошук аномалій та вирішення задачі *anomaly detection* за допомогою бібліотек `scikit-learn`та `PyTorch`**\n",
        "**Всі завдання виконуються індивідуально. Використання запозиченого коду буде оцінюватись в 0 балів.**\n",
        "\n",
        "**Лабораторні роботи де в коді буде використаня КИРИЛИЦІ будуть оцінюватись в 20 балів.**"
      ],
      "metadata": {
        "id": "1i8adKFQS7E-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Мета роботи:\n",
        "Ознайомитися з основними методами виявлення аномалій, навчитися використовувати бібліотеки `scikit-learn` та `PyTorch` для реалізації алгоритмів пошуку аномалій, проаналізувати ефективність різних методів на реальних наборах даних з Kaggle.\n",
        "\n",
        "\n",
        "### Опис завдання:\n",
        "\n",
        "1. **Постановка задачі**:\n",
        "   Використовуючи один із доступних наборів даних Kaggle (наприклад, *Credit Card Fraud Detection*, *Network Intrusion*, або інші), вам потрібно розв'язати задачу виявлення аномалій. Основна мета — ідентифікувати аномальні записи серед нормальних. Вибраний набір даних повинен містити мітки аномалій для перевірки результатів.\n",
        "\n",
        "2. **Етапи виконання завдання**:\n",
        "   - Завантажте та підготуйте набір даних.\n",
        "   - Проведіть попередню обробку даних (масштабування, заповнення пропущених значень, видалення нерелевантних ознак).\n",
        "   - Використайте різні методи виявлення аномалій:\n",
        "     - **Методи з бібліотеки scikit-learn**:\n",
        "       - Isolation Forest\n",
        "       - One-Class SVM\n",
        "       - Local Outlier Factor (LOF)\n",
        "     - **Методи з використанням PyTorch**:\n",
        "       - Автоенкодери для виявлення аномалій.\n",
        "   - Порівняйте отримані результати, обчисліть метрики якості (Precision, Recall, F1-Score).\n",
        "   - Оцініть, який метод найкраще підходить для вирішення задачі на вашому наборі даних.\n",
        "\n",
        "### Покрокова інструкція\n",
        "\n",
        "1. **Підготовка середовища**:\n",
        "   - Встановіть необхідні бібліотеки:\n",
        "     ```\n",
        "     pip install scikit-learn torch pandas numpy matplotlib\n",
        "     ```\n",
        "\n",
        "2. **Вибір набору даних з Kaggle**:\n",
        "   Зареєструйтесь на Kaggle та оберіть один із наборів даних для виявлення аномалій. Наприклад:\n",
        "   - [Credit Card Fraud Detection](https://www.kaggle.com/mlg-ulb/creditcardfraud)\n",
        "   - [Network Intrusion Detection](https://www.kaggle.com/xyuanh/benchmarking-datasets)\n",
        "\n",
        "3. **Попередня обробка даних**:\n",
        "   - Завантажте дані та проведіть їхню початкову обробку.\n",
        "   - Масштабуйте ознаки за допомогою `StandardScaler` або `MinMaxScaler`.\n",
        "   - Розділіть дані на навчальну і тестову вибірки.\n",
        "\n",
        "4. **Методи з бібліотеки `scikit-learn`**:\n",
        "\n",
        "   - **Isolation Forest**:\n",
        "     ```\n",
        "     from sklearn.ensemble import IsolationForest\n",
        "     ```\n",
        "\n",
        "   - **One-Class SVM**:\n",
        "     ```\n",
        "     from sklearn.svm import OneClassSVM\n",
        "     ```\n",
        "\n",
        "   - **Local Outlier Factor**:\n",
        "     ```\n",
        "     from sklearn.neighbors import LocalOutlierFactor\n",
        "     ```\n",
        "\n",
        "5. **Методи на основі нейронних мереж (PyTorch)**:\n",
        "\n",
        "   Використайте автоенкодер для пошуку аномалій. Побудуйте нейронну мережу з енкодером і декодером. Під час навчання порівняйте відновлені дані з вхідними та обчисліть помилку. Записи з великою помилкою можуть бути аномаліями.\n",
        "\n",
        "   - **Реалізація автоенкодера**:\n",
        "     ```\n",
        "     import torch\n",
        "     import torch.nn as nn\n",
        "     import torch.optim as optim\n",
        "     ```\n",
        "\n",
        "6. **Оцінка результатів**:\n",
        "   Використовуйте метрики оцінки якості:\n",
        "   - `Precision`, `Recall`, `F1-score`\n",
        "   ```\n",
        "   from sklearn.metrics import classification_report\n",
        "   ```\n",
        "\n",
        "7. **Звіт**:\n",
        "   - Поясніть, який метод дав найкращі результати.\n",
        "   - Проаналізуйте, чому деякі методи працюють краще на вашому наборі даних.\n",
        "   - Оцініть можливості використання глибоких нейронних мереж (автоенкодерів) для вирішення задачі.\n",
        "\n",
        "\n",
        "### Результати, які необхідно надати:\n",
        "1. Код рішення у вигляді Jupyter Notebook з аналізом результатів та поясненнями.\n",
        "\n",
        "\n",
        "### Дедлайн:\n",
        "[23 жовтня 23:59]\n",
        "\n",
        "\n",
        "### Корисні ресурси:\n",
        "- [Документація PyTorch](https://pytorch.org/docs/stable/index.html)\n",
        "- [Документація scikit-learn](https://scikit-learn.org/stable/documentation.html)\n",
        "- [Kaggle Datasets](https://www.kaggle.com/datasets)"
      ],
      "metadata": {
        "id": "YvneQUbQRRqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score, f1_score\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "NeqgMqm2UETO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('creditcard.csv')\n",
        "\n",
        "print(dataset.head())\n",
        "print(dataset.info())\n",
        "\n",
        "scaler = StandardScaler()\n",
        "dataset[['scaled_amount', 'scaled_time']] = scaler.fit_transform(dataset[['Amount', 'Time']])\n",
        "dataset.drop(['Amount', 'Time'], axis=1, inplace=True)\n",
        "\n",
        "features = dataset.drop('Class', axis=1)\n",
        "labels = dataset['Class']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    features, labels, test_size=0.2, random_state=42, stratify=labels\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pMZRM_O0U2n",
        "outputId": "d397b655-7523-4b03-cec3-5a5833ff0106"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
            "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
            "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
            "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
            "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
            "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
            "\n",
            "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
            "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
            "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
            "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
            "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
            "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
            "\n",
            "        V26       V27       V28  Amount  Class  \n",
            "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
            "1  0.125895 -0.008983  0.014724    2.69      0  \n",
            "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
            "3 -0.221929  0.062723  0.061458  123.50      0  \n",
            "4  0.502292  0.219422  0.215153   69.99      0  \n",
            "\n",
            "[5 rows x 31 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 284807 entries, 0 to 284806\n",
            "Data columns (total 31 columns):\n",
            " #   Column  Non-Null Count   Dtype  \n",
            "---  ------  --------------   -----  \n",
            " 0   Time    284807 non-null  float64\n",
            " 1   V1      284807 non-null  float64\n",
            " 2   V2      284807 non-null  float64\n",
            " 3   V3      284807 non-null  float64\n",
            " 4   V4      284807 non-null  float64\n",
            " 5   V5      284807 non-null  float64\n",
            " 6   V6      284807 non-null  float64\n",
            " 7   V7      284807 non-null  float64\n",
            " 8   V8      284807 non-null  float64\n",
            " 9   V9      284807 non-null  float64\n",
            " 10  V10     284807 non-null  float64\n",
            " 11  V11     284807 non-null  float64\n",
            " 12  V12     284807 non-null  float64\n",
            " 13  V13     284807 non-null  float64\n",
            " 14  V14     284807 non-null  float64\n",
            " 15  V15     284807 non-null  float64\n",
            " 16  V16     284807 non-null  float64\n",
            " 17  V17     284807 non-null  float64\n",
            " 18  V18     284807 non-null  float64\n",
            " 19  V19     284807 non-null  float64\n",
            " 20  V20     284807 non-null  float64\n",
            " 21  V21     284807 non-null  float64\n",
            " 22  V22     284807 non-null  float64\n",
            " 23  V23     284807 non-null  float64\n",
            " 24  V24     284807 non-null  float64\n",
            " 25  V25     284807 non-null  float64\n",
            " 26  V26     284807 non-null  float64\n",
            " 27  V27     284807 non-null  float64\n",
            " 28  V28     284807 non-null  float64\n",
            " 29  Amount  284807 non-null  float64\n",
            " 30  Class   284807 non-null  int64  \n",
            "dtypes: float64(30), int64(1)\n",
            "memory usage: 67.4 MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "anomaly_detector = IsolationForest(contamination=0.001, random_state=42)\n",
        "\n",
        "train_predictions = anomaly_detector.fit_predict(X_train)\n",
        "test_predictions = anomaly_detector.predict(X_test)\n",
        "test_predictions = [1 if prediction == -1 else 0 for prediction in test_predictions]\n",
        "\n",
        "print(\"Isolation Forest Results:\")\n",
        "print(classification_report(y_test, test_predictions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lu9LkaOG01cK",
        "outputId": "a4359f8d-83ca-4492-cb6f-64f60cb9857a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Isolation Forest Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.35      0.24      0.29        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.68      0.62      0.64     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "svm_anomaly_detector = OneClassSVM(gamma='auto', nu=0.001)\n",
        "\n",
        "svm_anomaly_detector.fit(X_train)\n",
        "svm_test_predictions = svm_anomaly_detector.predict(X_test)\n",
        "svm_test_predictions = [1 if prediction == -1 else 0 for prediction in svm_test_predictions]\n",
        "\n",
        "print(\"One-Class SVM Results:\")\n",
        "print(classification_report(y_test, svm_test_predictions))"
      ],
      "metadata": {
        "id": "78gKLxIe1Anp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c84c60d-6a9d-4f08-ae59-62b20a4ab3ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-Class SVM Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      0.99     56864\n",
            "           1       0.07      0.41      0.12        98\n",
            "\n",
            "    accuracy                           0.99     56962\n",
            "   macro avg       0.53      0.70      0.56     56962\n",
            "weighted avg       1.00      0.99      0.99     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lof_detector = LocalOutlierFactor(n_neighbors=20, contamination=0.001)\n",
        "\n",
        "lof_predictions = lof_detector.fit_predict(X_test)\n",
        "lof_predictions = [1 if prediction == -1 else 0 for prediction in lof_predictions]\n",
        "\n",
        "print(\"Local Outlier Factor Results:\")\n",
        "print(classification_report(y_test, lof_predictions))"
      ],
      "metadata": {
        "id": "4IBuifYp1J9Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb5a9552-5d13-4829-8f30-43fcc9bf0c13"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Local Outlier Factor Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00     56864\n",
            "           1       0.04      0.02      0.03        98\n",
            "\n",
            "    accuracy                           1.00     56962\n",
            "   macro avg       0.52      0.51      0.51     56962\n",
            "weighted avg       1.00      1.00      1.00     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalyAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(AnomalyAutoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(input_dim, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 8)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(8, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, input_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n",
        "\n",
        "input_features = X_train.shape[1]\n",
        "autoencoder_model = AnomalyAutoencoder(input_dim=input_features)\n",
        "\n",
        "loss_function = nn.MSELoss()\n",
        "optimizer = optim.Adam(autoencoder_model.parameters(), lr=0.001)\n",
        "\n",
        "train_tensor = torch.FloatTensor(X_train.values)\n",
        "test_tensor = torch.FloatTensor(X_test.values)\n",
        "\n",
        "epochs = 20\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    autoencoder_model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    reconstructed = autoencoder_model(train_tensor)\n",
        "    reconstruction_loss = loss_function(reconstructed, train_tensor)\n",
        "    reconstruction_loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    print(f'Epoch {epoch + 1}/{epochs}, Loss: {reconstruction_loss.item():.4f}')"
      ],
      "metadata": {
        "id": "zFlwJIRI1Sy_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1e9574f-6d3d-48d4-e16e-b1ac6ddcc488"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Loss: 1.0989\n",
            "Epoch 2/20, Loss: 1.0982\n",
            "Epoch 3/20, Loss: 1.0975\n",
            "Epoch 4/20, Loss: 1.0968\n",
            "Epoch 5/20, Loss: 1.0962\n",
            "Epoch 6/20, Loss: 1.0955\n",
            "Epoch 7/20, Loss: 1.0948\n",
            "Epoch 8/20, Loss: 1.0942\n",
            "Epoch 9/20, Loss: 1.0935\n",
            "Epoch 10/20, Loss: 1.0928\n",
            "Epoch 11/20, Loss: 1.0921\n",
            "Epoch 12/20, Loss: 1.0914\n",
            "Epoch 13/20, Loss: 1.0906\n",
            "Epoch 14/20, Loss: 1.0898\n",
            "Epoch 15/20, Loss: 1.0890\n",
            "Epoch 16/20, Loss: 1.0882\n",
            "Epoch 17/20, Loss: 1.0873\n",
            "Epoch 18/20, Loss: 1.0864\n",
            "Epoch 19/20, Loss: 1.0855\n",
            "Epoch 20/20, Loss: 1.0845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "autoencoder_model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_reconstructions = autoencoder_model(test_tensor)\n",
        "    reconstruction_losses = torch.mean((test_reconstructions - test_tensor) ** 2, dim=1)\n",
        "\n",
        "anomaly_threshold = torch.quantile(reconstruction_losses, 0.99).item()\n",
        "autoencoder_predictions = (reconstruction_losses > anomaly_threshold).int().numpy()\n",
        "\n",
        "print(\"Autoencoder Results:\")\n",
        "print(classification_report(y_test, autoencoder_predictions))"
      ],
      "metadata": {
        "id": "v-J5iPPZ1tVS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57a962e9-c283-46d3-b2e6-4e8395a1a6db"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autoencoder Results:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     56864\n",
            "           1       0.10      0.56      0.16        98\n",
            "\n",
            "    accuracy                           0.99     56962\n",
            "   macro avg       0.55      0.78      0.58     56962\n",
            "weighted avg       1.00      0.99      0.99     56962\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision_values = []\n",
        "recall_values = []\n",
        "f1_values = []\n",
        "\n",
        "detection_methods = ['Isolation Forest', 'One-Class SVM', 'LOF', 'Autoencoder']\n",
        "method_predictions = [y_test, svm_test_predictions, lof_predictions, autoencoder_predictions]\n",
        "\n",
        "for predictions in method_predictions:\n",
        "    precision_values.append(precision_score(y_test, predictions, zero_division=0))\n",
        "    recall_values.append(recall_score(y_test, predictions, zero_division=0))\n",
        "    f1_values.append(f1_score(y_test, predictions, zero_division=0))\n",
        "\n",
        "print(\"Performance Metrics by Method:\")\n",
        "for index, method in enumerate(detection_methods):\n",
        "    print(f\"{method}: Precision={precision_values[index]:.4f}, Recall={recall_values[index]:.4f}, F1-Score={f1_values[index]:.4f}\")"
      ],
      "metadata": {
        "id": "TyGPXWSf15eF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29b9d59-841b-4a34-ed57-061f07ce26e7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performance Metrics by Method:\n",
            "Isolation Forest: Precision=1.0000, Recall=1.0000, F1-Score=1.0000\n",
            "One-Class SVM: Precision=0.0677, Recall=0.4082, F1-Score=0.1161\n",
            "LOF: Precision=0.0351, Recall=0.0204, F1-Score=0.0258\n",
            "Autoencoder: Precision=0.0965, Recall=0.5612, F1-Score=0.1647\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Звіт\n",
        "\n",
        "1. **Мета:** Проведено аналіз даних із використанням різних алгоритмів виявлення аномалій:\n",
        "   - Isolation Forest\n",
        "   - One-Class SVM\n",
        "   - Local Outlier Factor (LOF)\n",
        "   - Autoencoder\n",
        "\n",
        "2. **Ключові кроки:**\n",
        "   - **Попередня обробка даних:** Масштабування та розділення на тренувальну і тестову вибірки.\n",
        "   - **Алгоритми:** Навчання та оцінка моделей для виявлення аномалій.\n",
        "   - **Метрики:** Розраховано Precision, Recall, F1-Score для кожного методу.\n",
        "\n",
        "3. **Результати:** Метрики продуктивності показали, що кожен метод має свої переваги та недоліки залежно від структури даних і параметрів.\n",
        "\n",
        "4. **Висновок:** Найкращий підхід варто обирати на основі контексту застосування, зважаючи на вимоги до точності, чутливості та швидкості моделі."
      ],
      "metadata": {
        "id": "MKe7YxOK3Xt8"
      }
    }
  ]
}